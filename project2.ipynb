{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project 2: Sequence Labeling (100 Points)\n",
    "=======================\n",
    "\n",
    "- This problem set, again based in part on one by Jacob Eisenstein, focuses on sequence labeling with Hidden Markov models and Deep Learning Models.\n",
    "- The target domain is part-of-speech tagging on English and Norwegian from the Universal Dependencies dataset.\n",
    "\n",
    "You will:\n",
    "- Do some basic preprocessing of the data\n",
    "- Build a naive classifier that tags each word with its most common tag\n",
    "- Implement a `Viterbi` Tagger using `Hidden Markov Model` in PyTorch\n",
    "- Build a `Bi-LSTM` deep learning model using PyTorch\n",
    "- Implement techniques to improve your classifier and compete with your classmates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup\n",
    "\n",
    "In order to develop this assignment, you will need [python 3.6](https://www.python.org/downloads/) and the following libraries. Most if not all of these are part of [anaconda](https://www.continuum.io/downloads), so a good starting point would be to install that.\n",
    "\n",
    "- [jupyter](http://jupyter.readthedocs.org/en/latest/install.html)\n",
    "- [numpy](https://docs.scipy.org/doc/numpy/user/install.html)\n",
    "- [matplotlib](http://matplotlib.org/users/installing.html)\n",
    "- [nosetests](https://nose.readthedocs.org/en/latest/)\n",
    "- [pandas](http://pandas.pydata.org/) Dataframes\n",
    "\n",
    "Here is some help on installing packages in python: https://packaging.python.org/installing/. You can use ```pip --user``` to install locally without sudo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this assignment\n",
    "\n",
    "- This is a Jupyter notebook. You can execute cell blocks by pressing control-enter.\n",
    "- Most of your coding will be in the python source files in the directory ```oswegonlp```.\n",
    "- The directory ```tests``` contains unit tests that will be used to grade your assignment, using ```nosetests```. You should run them as you work on the assignment to see that you're on the right track. You are free to look at their source code, if that helps -- though most of the relevant code is also here in this notebook. Learn more about running unit tests at http://pythontesting.net/framework/nose/nose-introduction/\n",
    "- You may want to add more tests, but that is completely optional. \n",
    "- **To submit this assignment, run the script ```make-submission.sh```, and submit the tarball ```project2-submission.tgz``` on Blackboard.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Python version\n",
      "python: 3.7.4 (default, Aug 13 2019, 20:35:49) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "print('My Python version')\n",
    "\n",
    "print('python: {}'.format(sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nose\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My library versions\n",
      "pandas: 1.1.4\n",
      "numpy: 1.16.2\n",
      "matplotlib: 3.1.1\n",
      "nose: 1.3.7\n",
      "torch: 1.8.1+cu102\n"
     ]
    }
   ],
   "source": [
    "print('My library versions')\n",
    "\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "print('nose: {}'.format(nose.__version__))\n",
    "print('torch: {}'.format(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test whether your libraries are the right version, run:\n",
    "\n",
    "`nosetests tests/test_environment.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\r\n",
      "----------------------------------------------------------------------\r\n",
      "Ran 1 test in 0.000s\r\n",
      "\r\n",
      "OK\r\n"
     ]
    }
   ],
   "source": [
    "# use ! to run shell commands in notebook\n",
    "! nosetests tests/test_environment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "# importing all the necessary files from oswegonlp\n",
    "from oswegonlp import constants, preprocessing, most_common, classifier_base, evaluation\n",
    "from oswegonlp import scorer, tagger_base, naive_bayes, hmm, viterbi, bilstm, cbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Processing\n",
    "The part-of-speech tags are defined on the [universal dependencies website](http://universaldependencies.org/en/pos/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(constants);\n",
    "## Define the file names\n",
    "TRAIN_FILE = constants.TRAIN_FILE\n",
    "DEV_FILE = constants.DEV_FILE\n",
    "TEST_FILE = constants.TEST_FILE \n",
    "TEST_FILE_HIDDEN = constants.TEST_FILE_UNLABELED\n",
    "NR_TRAIN_FILE = constants.NR_TRAIN_FILE\n",
    "NR_DEV_FILE = constants.NR_DEV_FILE\n",
    "NR_TEST_FILE = constants.NR_TEST_FILE\n",
    "NR_TEST_FILE_HIDDEN = constants.NR_TEST_FILE_UNLABELED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is demo code for using the function `conll_seq_generator(...)`. \n",
    "- The default value for max_insts is `1000000` indicating the num. of instances: and this should be enough for our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n"
     ]
    }
   ],
   "source": [
    "## Demo\n",
    "all_tags = set([])\n",
    "for (words, tags) in preprocessing.conll_seq_generator(TRAIN_FILE):\n",
    "    for tag in tags:\n",
    "        all_tags.add(tag)\n",
    "all_tags = sorted(all_tags)\n",
    "print (all_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 1.1**: Counting words per tag. (*10 points*)\n",
    "\n",
    "Implement the function `get_tag_word_counts` in `most_common.py`: The function should calculate the number of occurrences of all words for each tag.\n",
    "\n",
    "- **Input**: filename for data file, to be passed as argument to `preprocessing.conll_seq_generation`\n",
    "- **Output**: dict of counters, where keys are tags\n",
    "- **Tests**: ```test_most_common.py: test_get_top_noun_tags(),test_get_top_verb_tags()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(most_common);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN [('Bush', 211), ('US', 162), ('Iraq', 119)]\n",
      "PUNCT [('.', 8640), (',', 7021), ('-', 1314)]\n",
      "ADJ [('other', 268), ('good', 251), ('new', 195)]\n",
      "NOUN [('time', 385), ('people', 233), ('way', 187)]\n",
      "VERB [('have', 749), ('get', 359), ('know', 338)]\n",
      "DET [('the', 8141), ('a', 3589), ('The', 884)]\n",
      "ADP [('of', 3424), ('in', 2705), ('to', 1783)]\n",
      "AUX [('is', 1865), ('was', 1123), ('be', 1053)]\n",
      "PRON [('I', 3121), ('you', 1920), ('it', 1468)]\n",
      "PART [('to', 3221), ('not', 805), (\"n't\", 645)]\n",
      "SCONJ [('that', 983), ('if', 453), ('as', 312)]\n",
      "NUM [('one', 329), ('two', 157), ('2', 129)]\n",
      "ADV [('so', 371), ('just', 353), ('when', 306)]\n",
      "CCONJ [('and', 4843), ('or', 698), ('but', 600)]\n",
      "X [('etc', 39), ('1', 29), ('2', 29)]\n",
      "INTJ [('Please', 141), ('please', 111), ('Yes', 34)]\n",
      "SYM [('$', 251), ('-', 101), ('#', 37)]\n"
     ]
    }
   ],
   "source": [
    "# this block uses your code to find the three most common words per tag\n",
    "counters = most_common.get_tag_word_counts(TRAIN_FILE)\n",
    "for tag,tag_ctr in counters.items():\n",
    "    print (tag,tag_ctr.most_common(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Tagging as classification \n",
    "\n",
    "Now you will implement part-of-speech tagging via classification.\n",
    "\n",
    "Tagging quality is evaluated using evalTagger, which takes three arguments:\n",
    "- a tagger, which is a **function** taking a list of words and a tagset as arguments and returns the predicted tags for the words\n",
    "- an output filename\n",
    "- a test file\n",
    "\n",
    "You will want to use lambda expressions to create the first argument for the `eval_tagger(..)` function, as shown below.\n",
    "Here's how it works. I provide a tagger that labels everything as a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(tagger_base);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16667992047713717\n"
     ]
    }
   ],
   "source": [
    "# here is a tagger that just tags everything as a noun\n",
    "noun_tagger = lambda words, alltags : ['NOUN' for word in words]\n",
    "\n",
    "confusion = tagger_base.eval_tagger(noun_tagger,'nouns.preds',all_tags=all_tags)\n",
    "print (scorer.accuracy(confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Deliverable 2.1** Classification-based tagging. (*5 points*)\n",
    "\n",
    "Now do the same thing as above, but building your tagger *as a classifier.* To do this, implement `make_classifier_tagger()` in `tagger_base.py`. \n",
    "\n",
    "- **Input**: defaultdict of weights\n",
    "- **Output**: return a function that takes in (list of word tokens, list of all possible tags) $\\rightarrow$ tags for each word\n",
    "\n",
    "The function that you output should create the base-features for each token (**use the OFFSET and the TOKEN itself as base-features**)  and then use your `classifier_base.predict()` function from project 1. You are free to edit the `classifier_base.predict()` function if you don't think you got it right in project 1.\n",
    "- **Tests**: ```test_classifier_tagger.py:test_classifier()```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(tagger_base);\n",
    "reload(classifier_base);\n",
    "from oswegonlp.constants import OFFSET #OFFSET token is for each tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now create a tagger with weights that predict every token to be a NOUN. \n",
    "The function `get_noun_weights` is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_noun_tagger = tagger_base.make_classifier_tagger(most_common.get_noun_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16667992047713717\n"
     ]
    }
   ],
   "source": [
    "confusion = tagger_base.eval_tagger(classifier_noun_tagger,'all-nouns.preds',all_tags=all_tags)\n",
    "print (scorer.accuracy(confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Deliverable 2.2** Tagging words by their most common tag. (*5 points*)\n",
    "\n",
    "Now build a classifier tagger that tags each word with its most common tag in the training set. To do this, implement `get_most_common_word_weights` in `most_common.py`.  \n",
    "\n",
    "- **Input**: training file\n",
    "\n",
    "- **Output**: defaultdict of weights\n",
    "\n",
    "This function should return a set weights such that each word should get the tag that is most frequently associated with it in the training data. If the word does not appear in the training data, the weights should be set so that the tagger outputs the **most common tag** in the training data. For the out of vocabulary words, you need to think on how to set the weights so that you tag them by the most common tag.\n",
    "\n",
    "- **Tests**: ```test_classifier.py:test_mcc_tagger_output(), test_mcc_tagger_accuracy()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(most_common);\n",
    "theta_mc = most_common.get_most_common_word_weights(TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_mc = tagger_base.make_classifier_tagger(theta_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRON', 'AUX', 'AUX', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "tags = tagger_mc(['They','can','can','fish'],all_tags)\n",
    "print (tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DET', 'ADJ', 'NOUN', 'DET', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "tags = tagger_mc(['The','old','man','the','boat','.'],all_tags)\n",
    "print (tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Now let's run your tagger on the dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8559443339960239\n"
     ]
    }
   ],
   "source": [
    "confusion = tagger_base.eval_tagger(tagger_mc,'most-common.preds',all_tags=all_tags)\n",
    "print (scorer.accuracy(confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Naive Bayes as a tagger.\n",
    "    \n",
    "- You can use your Naive Bayes classifier form project 1 to set the weights for the classifier tagger. I have added a helper function `naive_bayes.get_nb_weights(..)`. Make sure to retain this function, when you copy your code.\n",
    "- If you don't think you got it right in project 1, you are free to change it now. If you got it right, then just examine the performance of naive bayes as tagger in the following blocks. There is no test or deliverable for this part.\n",
    "- Note that, for text classification, we had a bag of words feature vector and label for each document. For POS tagging, in order to estimate the weights for the classifier tagger, we will consider each token to be its own document. The following helper code converts the dataset to token level bag-of-words feature vector and labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(naive_bayes);\n",
    "nb_weights = naive_bayes.get_nb_weights(TRAIN_FILE, .01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This gives weights for each tag-word pair that represent $\\log P(word \\mid tag)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining vocab of words\n",
    "vocab = set([word for tag,word in nb_weights.keys() if word is not constants.OFFSET])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19672\n"
     ]
    }
   ],
   "source": [
    "print (len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999996653\n",
      "1.0000000000002178\n",
      "0.9999999999999724\n"
     ]
    }
   ],
   "source": [
    "print (sum(np.exp(nb_weights[('ADJ',word)]) for word in vocab))\n",
    "print (sum(np.exp(nb_weights[('NOUN',word)]) for word in vocab))\n",
    "print (sum(np.exp(nb_weights[('PUNCT',word)]) for word in vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have zero weights for OOV terms -- think about how this affects the classification here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print (nb_weights[('ADJ','baaaaaaaaad')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.184346549553986\n",
      "-2.964122263995488\n",
      "-2.3990689301958317\n"
     ]
    }
   ],
   "source": [
    "print (nb_weights[('VERB',constants.OFFSET)])\n",
    "print (nb_weights[('ADV',constants.OFFSET)])\n",
    "print (nb_weights[('PRON',constants.OFFSET)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offsets should correspond to log-probabilities $\\log P(y)$ such that $\\sum_y P(y) = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.exp(nb_weights[(tag,constants.OFFSET)]) for tag in all_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us look at the accuracy of our naive_bayes tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8424254473161034\n"
     ]
    }
   ],
   "source": [
    "confusion = tagger_base.eval_tagger(tagger_base.make_classifier_tagger(nb_weights),'nb-simple.preds')\n",
    "dev_acc = scorer.accuracy(confusion)\n",
    "print (dev_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice it's just about as good as the heuristic tagger from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Viterbi Algorithm\n",
    "\n",
    "In this section you will implement the Viterbi algorithm in **PyTorch**. To get warmed up, let's work out an example by hand. For simplicity, there are only two tags, **N**OUN and **V**ERB. Here are the parameters:\n",
    "\n",
    "| | Value |\n",
    "| ------------- |:-------------:|\n",
    "| $\\log P_E(\\cdot|N)$ | they: -1, can: -3, fish: -3 |\n",
    "| $\\log P_E(\\cdot|V)$ | they: -11, can: -2, fish: -4 |\n",
    "| $\\log P_T(\\cdot|N)$ | N: -5, V: -2, END: -2 |\n",
    "| $\\log P_T(\\cdot|V)$ | N: -1, V: -3, END: -3 |\n",
    "| $\\log P_T(\\cdot|\\text{START})$ | N :-1, V :-2 |\n",
    "\n",
    "where $P_E(\\cdot|\\cdot)$ is the emission probability and $P_T(\\cdot|\\cdot)$ is the transition probability.\n",
    " \n",
    "- In class we discussed the sentence *They can fish*. \n",
    "- Now work out a more complicated example: \"*They can can fish*\", where the second \"*can*\" refers to the verb of putting things into cans.\n",
    " \n",
    "**Deliverable 3.1** Work out the trellis by hand, and fill in the table in the file **```text-answers.md```** (*5 points*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Viterbi ##\n",
    "\n",
    "Here are some predefined weights, corresponding to the weights from the problem 3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = constants.START_TAG\n",
    "END_TAG = constants.END_TAG\n",
    "UNK = constants.UNK\n",
    "\n",
    "nb_weights={('NOUN','they'):-1,\\\n",
    "            ('NOUN','can'):-3,\\\n",
    "            ('NOUN','fish'):-3,\\\n",
    "            ('VERB','they'):-11,\\\n",
    "            ('VERB','can'):-2,\\\n",
    "            ('VERB','fish'):-4,}\n",
    "hmm_trans_weights={('NOUN','NOUN'):-5,\\\n",
    "                   ('VERB','NOUN'):-2,\\\n",
    "                   (END_TAG,'NOUN'):-2,\\\n",
    "                   ('NOUN','VERB'):-1,\\\n",
    "                   ('VERB','VERB'):-3,\\\n",
    "                   (END_TAG,'VERB'):-3,\\\n",
    "                   ('NOUN',START_TAG):-1,\\\n",
    "                   ('VERB',START_TAG):-2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 3.2** Complete the ```hmm.compute_weights_variables(...)``` function in `hmm.py` file (*5 points*).\n",
    "\n",
    "The function should basically convert the weights to respective pytorch variables .\n",
    "\n",
    "- **Inputs** :\n",
    "    - `nb_weights`: emission_weights: dictionary of weights\n",
    "    - `hmm_trans_weights`: dictionary of weights\n",
    "    - `vocab`: list of all the words\n",
    "    - `word_to_ix`: a dictionary that maps each word in the vocab to a unique index. **Does not have the OFFSET_TOKEN.**\n",
    "    - `tag_to_ix`: a dictionary that maps each tag (including the `START_TAG` and the `END_TAG`) to a unique index.  \n",
    "\n",
    "- **Outputs** : returns two torch Variables\n",
    "    - `emission_probs`: torch Variable of a matrix of size `Vocab x Tagset_size`: \n",
    "        such that for a specific weight say `(word1, tag1):value` would result in\n",
    "        `emission_probs[word_to_ix[word1]][tag_to_ix[tag1]]=value`, else a zero. Also, make sure to set weights such that `START_TAG` and `END_TAG` cannot generate any word. **Make sure to ignore the OFFSET weights that might be present in the nb_weights. Consider the words only in your word_to_ix.**\n",
    "        \n",
    "    - `tag_transition_probs`: torch Variable of a matrix of size `Tagset_size x Tagset_size`: \n",
    "        such that for a specific feature say `(tag1, tag2):value` \n",
    "        where tag1 is my succeeding tag, tag2 is my current tag. \n",
    "        This would result in `tag_transition_probs[tag_to_ix[tag1]][tag_to_ix[tag2]]=value`. \n",
    "        Also be sure to set the other weights such that there are no illegal transitions \n",
    "        (like from some tag to START_TAG -or- from END_TAG to some other tag)  \n",
    "\n",
    "- **Tests**: ```test_viterbi.py: test_compute_hmm_weights_variables()```\n",
    "\n",
    "Hint: Use `-np.inf` as weights for illegal transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below, observe that we are calculating `tag_to_ix, ix_to_tag, word_to_ix`. These are useful to access a particular emission score for a particular token and a tag. Look through the variables: tag_transition_probs and emission_probs below and it should be clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(hmm);\n",
    "word_to_ix={'they':0, 'can':1, 'fish':2, UNK:3}\n",
    "tag_to_ix = {START_TAG:0, 'NOUN':1, 'VERB':2, END_TAG:3}\n",
    "ix_to_tag = {v:k for k,v in tag_to_ix.items()}\n",
    "all_tags = [START_TAG, 'NOUN', 'VERB', END_TAG]\n",
    "words = ['they', 'can', 'fish']\n",
    "vocab = ['they','can','fish',UNK]\n",
    "# note that we are also including an UNK token: this will be helpful later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_probs, tag_transition_probs = hmm.compute_weights_variables(nb_weights, hmm_trans_weights, vocab, \n",
    "                                                                     word_to_ix, tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-inf, -inf, -inf, -inf],\n",
      "        [-1., -5., -1., -inf],\n",
      "        [-2., -2., -3., -inf],\n",
      "        [-inf, -2., -3., -inf]])\n"
     ]
    }
   ],
   "source": [
    "print (tag_transition_probs)\n",
    "# tag_transition_probs[0] corresponds to scores for START_TAG from START_TAG, NOUN, VERB, END_TAG\n",
    "# tag_transition_probs[1] corresponds to scores for NOUN from START_TAG, NOUN, VERB, END_TAG\n",
    "# tag_transition_probs[2] corresponds to scores for VERB from START_TAG, NOUN, VERB, END_TAG\n",
    "# tag_transition_probs[3] corresponds to scores for END_TAG from START_TAG, NOUN, VERB, END_TAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-inf,  -1., -11., -inf],\n",
      "        [-inf,  -3.,  -2., -inf],\n",
      "        [-inf,  -3.,  -4., -inf],\n",
      "        [-inf,   0.,   0., -inf]])\n"
     ]
    }
   ],
   "source": [
    "print (emission_probs)\n",
    "# emission_probs[0] corresponds to scores for the token 'they' for START_TAG, NOUN, VERB, END_TAG\n",
    "# emission_probs[1] corresponds to scores for the token 'can' for START_TAG, NOUN, VERB, END_TAG\n",
    "# emission_probs[2] corresponds to scores for the token 'fish' for START_TAG, NOUN, VERB, END_TAG\n",
    "# emission_probs[2] corresponds to scores for the token 'UNK' for START_TAG, NOUN, VERB, END_TAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we will be using these emission scores as inputs for each token in the input in the following function: ```viterbi_step()```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 3.3** The Viterbi recurrence. (*10 points*)\n",
    "\n",
    "Implement `viterbi_step` in `oswegonlp/viterbi.py`. This is the function that will compute the best path score and corresponding back pointer for a particular token in the sentence for all possible tags, which you will later call from the main viterbi routine. \n",
    "\n",
    "### Inputs\n",
    "- `all_tags`: list of all tags: includes both the `START_TAG` and the `END_TAG`\n",
    "- `tag_to_ix`: a dictionary that maps each tag (including the `START_TAG` and the `END_TAG`) to a unique index: this is useful to access the respective tag transition scores from the tag_transition_probs variable.\n",
    "- `cur_tag_scores`: pytorch Variable that contains the local emission score for each tag for the current token in the sentence.\n",
    "    - `cur_tag_scores` size is : `[ len(all_tags) ] `\n",
    "- `transition_scores`: pytorch Variable that contains the `tag_transition_scores`. \n",
    "    - `transition_scores` size is : `[ len(all_tags) x len(all_tags) ]` \n",
    "- `prev_scores`: pytorch Variable that contains the scores for each tag for the previous token in the sentence.\n",
    "    - `prev_scores` size is : `[ 1 x len(all_tags) ] `\n",
    "\n",
    "### Outputs\n",
    "- `viterbivars`: a pytorch Variable that contains the global scores for each tag for the current token in the sentence\n",
    "- `bptrs`: a list of idx that contains the best_previous_tag for each tag for the current token in the sentence\n",
    "\n",
    "### Tests\n",
    "- ```test_viterbi.py: test_viterbi_step_init()```  \n",
    "\n",
    "There are a lot of inputs, but the code itself will not be very complex. Make sure you understand what each input represents before starting to write a solution.\n",
    "\n",
    "**Do not convert the pytorch variables into numpy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Consider the sentence: `'they can can fish'`\n",
    "- Let us observe the viterbi scores at each of the tokens 'they', 'can', 'can', 'fish'.\n",
    "- We will walk through this example and all along: these scores should match with the scores you obtained when you worked it out by hand.\n",
    "- **Please note the dimensions of the tensors below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf]])\n"
     ]
    }
   ],
   "source": [
    "reload(viterbi);\n",
    "initial_vec = np.full((1, len(all_tags)),-np.inf) \n",
    "initial_vec[tag_to_ix[START_TAG]][0] = 0 #setting all the score to START_TAG\n",
    "prev_scores = viterbi.get_torch_variable(initial_vec)\n",
    "# these are the previous scores for each_tag: START_TAG, NOUN, VERB, END_TAG\n",
    "print (prev_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The block above says that the only possible previous tag at $m=1$ is `START_TAG`\n",
    "- Now let us look at the tag scores for the first token 'they'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--START--', 'NOUN', 'VERB', '--END--']\n"
     ]
    }
   ],
   "source": [
    "# make sure both START_TAG and END_TAG is included in all_tags\n",
    "print (all_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Carefully observe all the inputs to the `viterbi_step(..)` function here.\n",
    "    - `all_tags`: is the list of all possible tags here\n",
    "    - `tag_to_ix`: a mapping from tags to unique ids: this is useful to access the respective tag transition scores from the `tag_transition_probs` variable.\n",
    "    - `cur_tag_scores`: observe that from previous section: `emission_probs` indicates the emission scores for each tag for each word they: since we will be tagging the word 'they' right now in our example: we will be using ```emission_probs[0]```: note that '0' is the id for our word 'they'.Thus, we send in `emission_probs[0]` as our cur_tag_scores\n",
    "    - `tag_transition_probs`: tag transition probabilities\n",
    "    - `prev_scores`: prev_scores obtained: we have initially calculated these scores above such that the `START_TAG` has all the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(viterbi);\n",
    "viterbivars, bptrs = viterbi.viterbi_step(all_tags, tag_to_ix, \n",
    "                                          emission_probs[0], \n",
    "                                          tag_transition_probs,\n",
    "                                          prev_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The following are the scores obtained for each tag for the word token 'they' and the backpointer refers to that particular previous tag which resulted in that score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag:  --START--  score:  tensor(-inf)  back-pointer-tag:  --START--\n",
      "tag:  NOUN  score:  tensor(-2.)  back-pointer-tag:  --START--\n",
      "tag:  VERB  score:  tensor(-13.)  back-pointer-tag:  --START--\n",
      "tag:  --END--  score:  tensor(-inf)  back-pointer-tag:  --START--\n"
     ]
    }
   ],
   "source": [
    "scores = viterbivars\n",
    "for k,v in tag_to_ix.items():\n",
    "    print ('tag: ',k, ' score: ',scores[v], ' back-pointer-tag: ', \n",
    "           ix_to_tag[bptrs[v]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `NOUN` has the highest score for the current tag, and its backpointer is to `START_TAG`\n",
    "- Now, let us look at the scores for the tags for the second token 'can'. Send in `emission_probs[1]` as our `current_tag_scores`, and update `prev_scores` to be the scores obtained for $m=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prev_scores = viterbi.get_torch_variable([-np.inf, -2, -13, -np.inf])\n",
    "viterbivars, bptrs = viterbi.viterbi_step(all_tags, tag_to_ix,\n",
    "                                          emission_probs[1],\n",
    "                                          tag_transition_probs,\n",
    "                                          prev_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The following are the scores obtained for each tag for the word token 'can' and its respective back_pointer tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag:  --START--  score:  tensor(-inf)  back-pointer-tag:  --START--\n",
      "tag:  NOUN  score:  tensor(-10.)  back-pointer-tag:  NOUN\n",
      "tag:  VERB  score:  tensor(-6.)  back-pointer-tag:  NOUN\n",
      "tag:  --END--  score:  tensor(-inf)  back-pointer-tag:  --START--\n"
     ]
    }
   ],
   "source": [
    "scores = viterbivars\n",
    "for k,v in tag_to_ix.items():\n",
    "    print ('tag: ',k, ' score: ',scores[v], ' back-pointer-tag: ',\n",
    "           ix_to_tag[bptrs[(all_tags).index(k)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, Below, let us look at the scores for the tags for the third token 'can'. So, now we send in `emission_probs[1]` as our `current_tag_scores` and we update `prev_scores` to be the scores obtained for the previous token 'can'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_scores = viterbi.get_torch_variable([-np.inf, -10, -6, -np.inf]) \n",
    "viterbivars, bptrs = viterbi.viterbi_step(all_tags, tag_to_ix,\n",
    "                                          emission_probs[1],\n",
    "                                          tag_transition_probs,\n",
    "                                          prev_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag:  --START--  score:  tensor(-inf)  back-pointer-tag:  --START--\n",
      "tag:  NOUN  score:  tensor(-10.)  back-pointer-tag:  VERB\n",
      "tag:  VERB  score:  tensor(-11.)  back-pointer-tag:  VERB\n",
      "tag:  --END--  score:  tensor(-inf)  back-pointer-tag:  --START--\n"
     ]
    }
   ],
   "source": [
    "scores = viterbivars\n",
    "for k,v in tag_to_ix.items():\n",
    "    print ('tag: ',k, ' score: ',scores[v], ' back-pointer-tag: ',\n",
    "           ix_to_tag[bptrs[(all_tags).index(k)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Now, let us look at the scores for the tags for the last token 'fish', So, now we send in `emission_probs[2]` as our `current_tag_scores` and we update `prev_scores` to be the scores obtained above for the previous token 'can'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_scores = viterbi.get_torch_variable([-np.inf, -10, -11, -np.inf])\n",
    "viterbivars, bptrs = viterbi.viterbi_step(all_tags, tag_to_ix,\n",
    "                                           emission_probs[2],\n",
    "                                           tag_transition_probs,\n",
    "                                           prev_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag:  --START--  score:  tensor(-inf)  back-pointer-tag:  --START--\n",
      "tag:  NOUN  score:  tensor(-15.)  back-pointer-tag:  VERB\n",
      "tag:  VERB  score:  tensor(-16.)  back-pointer-tag:  NOUN\n",
      "tag:  --END--  score:  tensor(-inf)  back-pointer-tag:  --START--\n"
     ]
    }
   ],
   "source": [
    "scores = viterbivars\n",
    "for k,v in tag_to_ix.items():\n",
    "    print ('tag: ',k, ' score: ',scores[v], ' back-pointer-tag: ',\n",
    "           ix_to_tag[bptrs[(all_tags).index(k)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 3.4** Build the Viterbi trellis. (*10 points*)\n",
    "\n",
    "This function should compute the `best_path` and the `path_score`. This function takes in the `emission_scores` for each particular token in the sentence, the `tag_transitions_weight` and returns the best set of tags for that particular sequence. Use `viterbi_step` to implement `build_trellis` in `viterbi.py` in Pytorch. \n",
    "\n",
    "This function should take:\n",
    "- **Inputs**:\n",
    "    - `all_tags`: a list of all tags: includes START_TAG and END_TAG\n",
    "    - `tag_to_ix`: a dictionary that maps each tag to a unique id.\n",
    "    - `cur_tag_scores`: a list of pytorch Variables where each contains the local emission score for each tag for that particular token in the sentence, len(cur_tag_scores) will be equal to len(words): each pytorch variables size would be equal to len(all_tags) indicating the score for each_tag.\n",
    "    - `transition_scores`: pytorch Variable (a matrix) that contains the tag_transition_scores\n",
    "\n",
    "- **Outputs**:\n",
    "    - `path_score`: the score for the best_path\n",
    "    - `best_path`: the actual best_path, which is the list of tags for each token: exclude the `START_TAG` and `END_TAG` here.   \n",
    "    \n",
    "- **Tests**: ```test_viterbi.py: test_trellis_score(), test_build_trellis()```\n",
    "First, make sure to pass the ```test_trellis_score()``` test and then move on to the ```test_build_trellis()``` test.\n",
    "\n",
    "** Note that for the input cur_tag_scores: we are sending in a list of pytorch variables: one for each token in the sentence to be tagged **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--START--', 'NOUN', 'VERB', '--END--']\n"
     ]
    }
   ],
   "source": [
    "# make sure START_TAG and END_TAG are in all_tags\n",
    "print (all_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- consider the same sentence as above: 'they can can fish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['they', 'can', 'can', 'fish']\n",
      "{'they': 0, 'can': 1, 'fish': 2, '<UNK>': 3}\n"
     ]
    }
   ],
   "source": [
    "words = 'they can can fish'.split()\n",
    "print (words)\n",
    "print (word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below, we create `cur_tag_scores` using the `emission_probs` for each word in the sentence `'they can can fish'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing cur_tag_scores for the above sentence 'they can can fish'\n",
    "cur_tag_scores=[emission_probs[0],emission_probs[1],emission_probs[1],emission_probs[2]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complete the code to obtain the correct path_score initially and then use the backpointers to obtain the best_path. \n",
    "- Observe the inputs we are sending in for our example: 'they can can fish'\n",
    "    - `all_tags`: list of all tags including the `START_TAG` and `END_TAG`\n",
    "    - `tag_to_ix`: a mapping from tags to their unique ids\n",
    "    - `cur_tag_scores`: a list of pytorch variables: where each one is the score for each tag for a particular token. We send in these scores for each token in the sentence.\n",
    "    - `tag_transition_probs`: tag transition probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(viterbi);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_score, best_path = viterbi.build_trellis(all_tags, \n",
    "                                                  tag_to_ix, \n",
    "                                                  cur_tag_scores, \n",
    "                                                  tag_transition_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN', 'VERB', 'VERB', 'NOUN']\n",
      "tensor(-17.)\n"
     ]
    }
   ],
   "source": [
    "print (best_path)\n",
    "print (path_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN', 'VERB', 'VERB', 'NOUN', 'VERB', 'NOUN'] -25.0\n"
     ]
    }
   ],
   "source": [
    "sentence = ['they','can','can','can','can','fish']\n",
    "cur_tag_scores = [emission_probs[word_to_ix[w]] for w in sentence]\n",
    "\n",
    "path_score, best_path = viterbi.build_trellis(all_tags, \n",
    "                                                  tag_to_ix, \n",
    "                                                  cur_tag_scores, \n",
    "                                                  tag_transition_probs)\n",
    "print (best_path, path_score.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hidden Markov Model: Estimation\n",
    "\n",
    "You will now implement the estimatation for a hidden Markov model.\n",
    "\n",
    "We'll start with the tag transitions.\n",
    "\n",
    "**Deliverable 4.1** (*5 points*) Complete the function `most_common.get_tag_trans_counts()`.  This function should get the tag transition counts from the each tag to all possible tags. Don't forget to add the transitions from the `START_TAG` and the transitions from the `END_TAG`.\n",
    "\n",
    "You should use the `preprocessing.conll_seq_generator()` function.  \n",
    "\n",
    "- **Inputs**: `trainfile`, name of file containing training data\n",
    "- **Outputs**: a dictionary where keys are current tags and values are counters of succeeding tags.\n",
    "- **Tests**: ```test_hmm_trans_counts.py: test_tag_trans_counts()```  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(most_common);\n",
    "tag_trans_counts = most_common.get_tag_trans_counts(TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a dict of counters, where the keys are tags.\n",
    "\n",
    "Each counter is the frequency of tags following a given tag, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NOUN': 9671, 'ADJ': 3605, 'PROPN': 1488, 'VERB': 308, 'NUM': 252, 'ADV': 251, 'PUNCT': 201, 'ADP': 185, 'DET': 165, 'PRON': 65, 'SYM': 35, 'AUX': 29, 'X': 14, 'CCONJ': 10, 'PART': 4, 'SCONJ': 1})\n",
      "Counter({'PRON': 3533, 'PROPN': 1466, 'DET': 1258, 'ADV': 998, 'NOUN': 782, 'VERB': 764, 'ADP': 535, 'ADJ': 486, 'SCONJ': 445, 'PUNCT': 433, 'NUM': 406, 'INTJ': 398, 'AUX': 358, 'CCONJ': 290, 'X': 242, 'SYM': 106, 'PART': 43})\n"
     ]
    }
   ],
   "source": [
    "print (tag_trans_counts['DET'])\n",
    "print (tag_trans_counts[START_TAG])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 4.2** Estimate transition log-probabilities for an HMM. (*5 points*)\n",
    "\n",
    "Implement `compute_transition_weights` in `hmm.py`. This function should return a dictionary of weights such that ```weights[(tag2,tag1)]``` indicates the weights for transitions from `tag1` $\\rightarrow$ `tag2`. These weights will be used later for the Viterbi Tagger.\n",
    "\n",
    "### Inputs\n",
    "- Transition counts (generated from `get_tag_trans_counts`)\n",
    "- Smoothing\n",
    "\n",
    "### Outputs\n",
    "- Defaultdict with weights for transition features, in the form $[(y_m,y_{m-1})]$\n",
    "\n",
    "### Tests\n",
    "```test_hmm_trans.py: test_hmm_trans_weights_sum_to_one(), test_hmm_trans_weights_exact_vals() ```  \n",
    "\n",
    "Hints: \n",
    "\n",
    "- Don't forget to assign smoothed probabilities to transitions which do not appear in the counts. \n",
    "- Do not assign probabilities for transitions to the `START_TAG`, which can only come first. This will also affect your computation of the denominator, since you are not smoothing the probability of transitions to the `START_TAG`.\n",
    "- Don't forget to assign probabilities to transitions to the `END_TAG`; this too will affect your denominator.\n",
    "- As always, probabilities should sum to one (this time conditioned on the previous tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(hmm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_trans_weights = hmm.compute_transition_weights(tag_trans_counts,.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782 -2.7750635154957806\n",
      "764 -2.798350436746465\n",
      "308 -3.967836446690126\n",
      "0 -16.605694755393813\n",
      "9671 -0.5210523772927953\n",
      "0 -inf\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print (tag_trans_counts[START_TAG]['NOUN'], hmm_trans_weights[('NOUN',START_TAG)])\n",
    "print (tag_trans_counts[START_TAG]['VERB'], hmm_trans_weights[('VERB',START_TAG)])\n",
    "print (tag_trans_counts['DET']['VERB'], hmm_trans_weights[('VERB','DET')])\n",
    "print (tag_trans_counts['DET']['INTJ'], hmm_trans_weights[('INTJ','DET')])\n",
    "print (tag_trans_counts['DET']['NOUN'], hmm_trans_weights[('NOUN','DET')])\n",
    "print (tag_trans_counts['VERB'][START_TAG], hmm_trans_weights[(START_TAG,'VERB')])\n",
    "#print (tag_trans_counts[END_TAG]['VERB']) # will throw key error\n",
    "print (hmm_trans_weights[('VERB',END_TAG)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These log-probabilities should normalize to when summing over $y_m$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999712147544\n",
      "0.999998444836\n",
      "0.9999999198911732\n"
     ]
    }
   ],
   "source": [
    "#calculating all tags here, we also add END_TAG here.\n",
    "all_tags = sorted(list(tag_trans_counts.keys()) + [END_TAG])\n",
    "print (sum(np.exp(hmm_trans_weights[(tag,'NOUN')]) for tag in all_tags))\n",
    "print (sum(np.exp(hmm_trans_weights[(tag,'SYM')]) for tag in all_tags))\n",
    "print (sum(np.exp(hmm_trans_weights[(tag,'ADJ')]) for tag in all_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let us compute the weight variables for the whole dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recalculating nb_weights for the whole dataset\n",
    "nb_weights = naive_bayes.get_nb_weights(TRAIN_FILE, .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'--END--': 0, '--START--': 1, 'ADJ': 2, 'ADP': 3, 'ADV': 4, 'AUX': 5, 'CCONJ': 6, 'DET': 7, 'INTJ': 8, 'NOUN': 9, 'NUM': 10, 'PART': 11, 'PRON': 12, 'PROPN': 13, 'PUNCT': 14, 'SCONJ': 15, 'SYM': 16, 'VERB': 17, 'X': 18}\n"
     ]
    }
   ],
   "source": [
    "# recalculating tag_to_ix={}\n",
    "tag_to_ix={}\n",
    "for tag in list(all_tags):\n",
    "    if tag not in tag_to_ix:\n",
    "        tag_to_ix[tag]=len(tag_to_ix)\n",
    "print (tag_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Note about OOV's**: We provide a helper function to calculate the `vocab` as shown below. We add an `UNK` token to the `vocab`. This is useful because, when we don't find a token's emission weight, we choose the `UNK` tokens weight and proceed with our tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19673\n"
     ]
    }
   ],
   "source": [
    "# recalculating vocab for the whole dataset now. # we also add an UNK token to the vocab here\n",
    "reload(most_common);\n",
    "vocab, word_to_ix = most_common.get_word_to_ix(TRAIN_FILE) #obtains all the words in the file\n",
    "print (len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_probs, tag_transition_probs = hmm.compute_weights_variables(nb_weights, hmm_trans_weights, \n",
    "                                                                     vocab, word_to_ix, tag_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 4.3** (*5 points*)\n",
    "\n",
    "We can now combine `Viterbi` and the `HMM` weights to compute the tag sequence for the example sentence. Make sure your implementation passes the test for this deliverable, and **explain (in `text-answers.md`) whether you think these predicted tags are correct**, based on your understanding of the universal part-of-speech tag set.\n",
    "\n",
    "- **Tests**: ```test_hmm.py: test_hmm_on_example_sentence()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--END--', '--START--', 'ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n"
     ]
    }
   ],
   "source": [
    "# make sure all_tags has END_TAG\n",
    "print (all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-28.5981), ['PRON', 'AUX', 'AUX', 'ADJ', 'PUNCT'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(viterbi);\n",
    "viterbi.build_trellis(all_tags,\n",
    "                      tag_to_ix,\n",
    "                      [emission_probs[word_to_ix[w]] for w in ['they', 'can', 'can', 'fish','.']], \n",
    "                      tag_transition_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 4.4** (*2.5 points*)\n",
    "\n",
    "- Run your HMM tagger on the dev data and test data, using the code blocks below.\n",
    "- **Tests**: ```test_hmm.py: test_hmm_dev_accuracy(), test_hmm_test_accuracy()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observe that, based on our definition of the viterbi function, we need to send in two sets of important scores to the `build_trellis()` function\n",
    "    - cur_tag_scores: a list of emission scores for each tag for each token in the sentence\n",
    "    - tag_transition_probs: tag transition scores\n",
    "- When using the `HMM` with `Viterbi` Tagger: we have calculated the `cur_tag_scores` using the `naive_bayes_weights` and `tag_transition_probs` in 4.1.\n",
    "- As I have already mentioned above, for `cur_tag_scores`, we are sending in a list of pytorch variables: one for each token in the sentence to be tagged\n",
    "- Below, in the tagger that we create: we first calculate the set of `cur_tag_scores` for the words in the sentence and then send them in. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(viterbi);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observe the way `cur_tag_scores` is computed in the loop below: \n",
    "    - For each particular word in a sentence: we assign the respective emission scores if it is present in our `vocab`, else we assign the emission_scores of an `UNK` token.\n",
    "    - This is repeated everywhere from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From/ADP the/DET AP/NOUN comes/VERB this/DET story/NOUN :/PUNCT \n",
      "\n",
      "President/PROPN Bush/PROPN on/ADP Tuesday/PROPN nominated/VERB two/NUM individuals/NOUN to/PART replace/VERB retiring/PART jurists/VERB on/ADP federal/ADJ courts/NOUN in/ADP the/DET Washington/PROPN area/NOUN ./PUNCT \n",
      "\n",
      "Bush/PROPN nominated/VERB Jennifer/PROPN M./PROPN Anderson/PROPN for/ADP a/DET 15/NUM -/PUNCT year/NOUN term/NOUN as/ADP associate/NOUN judge/NOUN of/ADP the/DET Superior/PROPN Court/PROPN of/ADP the/DET District/PROPN of/ADP Columbia/PROPN ,/PUNCT replacing/VERB Steffen/ADP W./PROPN Graae/PROPN ./PUNCT \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is just for fun\n",
    "for i,(words,_) in enumerate(preprocessing.conll_seq_generator(DEV_FILE)):\n",
    "    cur_tag_scores = [emission_probs[word_to_ix[w]] \n",
    "                      if w in word_to_ix else emission_probs[word_to_ix[UNK]] for w in words]\n",
    "    \n",
    "    pred_tags = viterbi.build_trellis(all_tags,\n",
    "                                      tag_to_ix,\n",
    "                                      cur_tag_scores,\n",
    "                                      tag_transition_probs)[1]\n",
    "    for word,pred_tag in zip(words,pred_tags):\n",
    "        print (\"%s/%s\"%(word,pred_tag),end=\" \")\n",
    "    print ('\\n')\n",
    "    if i >= 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = lambda words, all_tags : viterbi.build_trellis(all_tags, \n",
    "                                                        tag_to_ix,\n",
    "                                                            [emission_probs[word_to_ix[w]] \n",
    "                                                             if w in word_to_ix \n",
    "                                                             else emission_probs[word_to_ix[UNK]] \n",
    "                                                             for w in words],\n",
    "                                                            tag_transition_probs)[1]\n",
    "confusion = tagger_base.eval_tagger(tagger,'hmm-dev-en.preds', all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8790457256461233\n"
     ]
    }
   ],
   "source": [
    "print (scorer.accuracy(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_base.apply_tagger(tagger,'hmm-te-en.preds',all_tags, testfile=TEST_FILE_HIDDEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/en_ewt-ud-test.conllu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-25eadfdb0d50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# you don't have en-ud-test.conllu, so you can't run this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mte_confusion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_confusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'hmm-te-en.preds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte_confusion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Freelance/Linyung-python-ds-alg-exam/project 2/Project2/oswegonlp/scorer.py\u001b[0m in \u001b[0;36mget_confusion\u001b[0;34m(keyfilename, responsefilename)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \"\"\"\n\u001b[1;32m     25\u001b[0m     \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeyfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponsefilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeyfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;31m# Force opening of the file in binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/en_ewt-ud-test.conllu'"
     ]
    }
   ],
   "source": [
    "# you don't have en-ud-test.conllu, so you can't run this\n",
    "te_confusion = scorer.get_confusion(TEST_FILE,'hmm-te-en.preds')\n",
    "print (scorer.accuracy(te_confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech Tagging in Norwegian\n",
    "**Deliverable 4.5** (*2.5 points*)\n",
    "- Now, let us do part of speech tagging for data in Norwegian language using the Viterbi Tagger.\n",
    "- **Tests**: ```test_hmm.py: test_nr_hmm_dev_accuracy(), test_nr_hmm_test_accuracy()``` (you can't run this last one!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, we calculate the `nb_weights`/emission weights for the norwegian language in a similar way as we did for the english language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recalculating nb_weights for the whole dataset\n",
    "nb_weights_nr = naive_bayes.get_nb_weights(NR_TRAIN_FILE, .01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we calculate the `tag_transition_weights` for the norwegian language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_trans_counts_nr = most_common.get_tag_trans_counts(NR_TRAIN_FILE)\n",
    "hmm_trans_weights_nr = hmm.compute_transition_weights(tag_trans_counts_nr,.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we obtain the `vocab`, `word_to_ix` and `tag_to_ix` below for the norwegian language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'--END--': 0, '--START--': 1, 'ADJ': 2, 'ADP': 3, 'ADV': 4, 'AUX': 5, 'CCONJ': 6, 'DET': 7, 'INTJ': 8, 'NOUN': 9, 'NUM': 10, 'PART': 11, 'PRON': 12, 'PROPN': 13, 'PUNCT': 14, 'SCONJ': 15, 'SYM': 16, 'VERB': 17, 'X': 18}\n"
     ]
    }
   ],
   "source": [
    "#Using helper functions to obtain vocab, word_to_ix, tag_to_ix\n",
    "all_tags_nr = sorted(list(tag_trans_counts_nr.keys()) + [END_TAG])\n",
    "vocab_nr, word_to_ix_nr = most_common.get_word_to_ix(NR_TRAIN_FILE) #obtains all the words in the vocab\n",
    "tag_to_ix_nr={}\n",
    "for tag in list(all_tags_nr):\n",
    "    tag_to_ix_nr[tag]=len(tag_to_ix_nr)\n",
    "print (tag_to_ix_nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we convert these weights into pytorch variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_probs_nr, tag_transition_probs_nr = hmm.compute_weights_variables(nb_weights_nr, hmm_trans_weights_nr, \n",
    "                                                                           vocab_nr, word_to_ix_nr, tag_to_ix_nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we construct a viterbi tagger for the norwegian language using these weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = lambda words, all_tags : viterbi.build_trellis(all_tags_nr, \n",
    "                                                           tag_to_ix_nr,\n",
    "                                                            [emission_probs_nr[word_to_ix_nr[w]] \n",
    "                                                             if w in word_to_ix_nr \n",
    "                                                             else emission_probs_nr[word_to_ix_nr[UNK]] \n",
    "                                                             for w in words],\n",
    "                                                            tag_transition_probs_nr)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = tagger_base.eval_tagger(tagger,'hmm-dev-nr.preds', all_tags_nr,\n",
    "                                    trainfile=NR_TRAIN_FILE,\n",
    "                                    testfile=NR_DEV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9131953036927053\n"
     ]
    }
   ],
   "source": [
    "print (scorer.accuracy(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_base.apply_tagger(tagger,'hmm-te-nr.preds',all_tags_nr, \n",
    "                         trainfile=NR_TRAIN_FILE, testfile=NR_TEST_FILE_HIDDEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/no_bokmaal-ud-test.conllu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-d6b8a1e28724>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# you don't have no_bokmaal-ud-test.conllu, so you can't run this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mte_confusion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_confusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNR_TEST_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'hmm-te-nr.preds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte_confusion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Freelance/Linyung-python-ds-alg-exam/project 2/Project2/oswegonlp/scorer.py\u001b[0m in \u001b[0;36mget_confusion\u001b[0;34m(keyfilename, responsefilename)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \"\"\"\n\u001b[1;32m     25\u001b[0m     \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeyfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponsefilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeyfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, encoding, errors, buffering)\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;31m# Force opening of the file in binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/no_bokmaal-ud-test.conllu'"
     ]
    }
   ],
   "source": [
    "# you don't have no_bokmaal-ud-test.conllu, so you can't run this\n",
    "te_confusion = scorer.get_confusion(NR_TEST_FILE,'hmm-te-nr.preds')\n",
    "print (scorer.accuracy(te_confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. BiLSTM model for Part of Speech Tagging\n",
    "\n",
    "A `BiLSTM` model for part-of-speech tagging takes as input the word embeddings of the tokens in the sentence, and passes them through an `LSTM`. For each token, the hidden state is used as input to a network that computes a score for each tag. A softmax layer then converts these scores to probabilities. This model should be trained end-to-end with the cross-entropy loss function.\n",
    "\n",
    "We will be building this `BiLSTM` model as a class using pytorch. Your implementation will include three functions:\n",
    "\n",
    "- `BiLSTM.__init__()`: define all the necessary model parameters\n",
    "    1. The word-embedding matrix, which maps the words to vectors\n",
    "    2. A BiLSTM Neural Network, which takes the word embeddings for the words as inputs and produces a hidden state for each token.\n",
    "    3. A one layer feedforward Neural Network, which projects the hidden state to a vector of scores for each tag\n",
    "- `forward()`: pass the input through the model, obtaining probability distributions over tags\n",
    "    1. Convert all the words to their word-vectors from the word-embedding matrix\n",
    "    2. Pass these word-vectors through a BiLSTM to obtain hidden states for the tokens\n",
    "    3. Pass these hidden states through the feedforward neural network to obtain the probability distributions of tags for each token.\n",
    "- `cross_entopy_loss()`, the training objective\n",
    "\n",
    "The description below provides additional help for each of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words in the vocabulary:  6900\n",
      "6899\n"
     ]
    }
   ],
   "source": [
    "# recalculating vocab: obtains the most common 6900 words from the file\n",
    "vocab, word_to_ix = most_common.get_word_to_ix(TRAIN_FILE, 6900)\n",
    "print ('words in the vocabulary: ',len(word_to_ix))\n",
    "print (word_to_ix[UNK])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- updating `tag_to_ix` and `all_tags` to remove `START_TAG` and `END_TAG`: these labels are not necessary in tagging with `BiLSTM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "if START_TAG in all_tags:\n",
    "    all_tags.remove(START_TAG)\n",
    "if END_TAG in all_tags:\n",
    "    all_tags.remove(END_TAG)\n",
    "tag_to_ix={}\n",
    "for tag in all_tags:\n",
    "    if tag not in tag_to_ix:\n",
    "        tag_to_ix[tag] = len(tag_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Take a look at the helper functions `preprocessing.load_data(...)` and `bilstm.prepare_sequence(...)` \n",
    "    - `preprocessing.load_data(...)`: loads the data into a list of lists\n",
    "    - `bilstm.prepare_sequence(...)` given a sequence of words/tags and the `to_ix` dictionary that maps them to its unique indices: it returns a sequence of its unique indices.\n",
    "- The function `prepare_sequence()` will be used a lot from now on to convert the input to a `torch.LongTensor` and then send it to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Training data for english:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  26 1052   17  155  438   42    1  593    6   50 1979 1343    0]\n",
      "[ 5  0  3  3 15  1  5  7  1  5  7  7 12]\n"
     ]
    }
   ],
   "source": [
    "reload(preprocessing);\n",
    "X_tr, Y_tr = preprocessing.load_data(TRAIN_FILE)\n",
    "\n",
    "print (bilstm.prepare_sequence(X_tr[5],word_to_ix).data.numpy())\n",
    "print (bilstm.prepare_sequence(Y_tr[5],tag_to_ix).data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dev data for english:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dv, Y_dv = preprocessing.load_data(DEV_FILE)\n",
    "# loading dev data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Test data for english:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te, Y_te = preprocessing.load_data(TEST_FILE_HIDDEN)\n",
    "# loading test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 5.1**: (*7.5 points*)\n",
    "- Complete the `__init__()` function in the Class `bilstm.BiLSTM` that defines the model parameters:\n",
    "    - **Inputs**:\n",
    "        - `vocab_size`: vocab size of the model\n",
    "        - `tag_to_ix`: tag_to_ix: a dictionary that maps the tags to its unique id\n",
    "        - `embedding_dim`: embedding dimension for the words\n",
    "        - `hidden_dim`: hidden dimension for the `Bi-LSTM` model\n",
    "        - `embeddings`: embedding matrix of size: vocab_size x embedding_dim (this is to initialize embeddings with pretrained embeddings).\n",
    "    - The function does the following:\n",
    "        - Create an embedding matrix using torch.nn.Embedding of the size vocab \n",
    "            - [check this pytorch_doc_for_embedding](https://pytorch.org/docs/1.3.1/nn.html?highlight=embedding#torch.nn.Embedding)\n",
    "        - Create a Bi-LSTM model with just one layer, and hidden dimension = hidden_dim \n",
    "            - [check this pytorch_doc_for_LSTM](http://pytorch.org/docs/1.3.1/nn.html?highlight=lstm#torch.nn.LSTM)\n",
    "        - Also add a FullyConnected Layer, that would project the hidden state onto the tag space. \n",
    "            - [check this pytorch_doc_for_Linear](http://pytorch.org/docs/1.3.1/nn.html?highlight=linear#torch.nn.Linear)\n",
    "    - Make sure to name the parameters as follows: The unit tests will check for these variables\n",
    "        - `self.word_embeds`\n",
    "        - `self.lstm`\n",
    "        - `self.hidden2tag`\n",
    "    - **Tests**: ```test_bilstm.py: test_dlmodel_init()```\n",
    "    - All you need to do here is to define the model parameters here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below, you can find brief example of how these components are created in Torch. In these examples, the dimensions are arbitrary; you will need to determine the correct dimensions for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this basically defines a matrix of embeddings where the vocab_size=10 and the embedding_dim=10 \n",
    "word_embeds = nn.Embedding(num_embeddings=10, embedding_dim=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following line is how you define an LSTM such that the input_size of the vector=10, \n",
    "# hidden state of each LSTM (forward and backward) =20, num of layers=1, \n",
    "# bidirectional=True indicating both forward and backward LSTM will be included.\n",
    "lstm = nn.LSTM(input_size=10, hidden_size = 20, num_layers=1, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following line is the way to define a Fully Connected Layer with input_dim=40, output_dim=10 and bias=True.\n",
    "hidden2tag = nn.Linear(in_features=40, out_features=10, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once, you have defined the parameters of the model: check if you have done it right using the unit test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bilstm);\n",
    "torch.manual_seed(765);\n",
    "embedding_dim=30\n",
    "hidden_dim=30\n",
    "model = bilstm.BiLSTM(len(word_to_ix),tag_to_ix,embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (word_embeds): Embedding(6900, 30)\n",
       "  (lstm): LSTM(30, 15, bidirectional=True)\n",
       "  (hidden2tag): Linear(in_features=30, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 5.2** (*7.5 points*)\n",
    "- Complete the `bilstm.BiLSTM.forward()` function in the `bilstm.BiLSTM` class to obtain the scores for each tag for each of the words in a sentence\n",
    "- **Input**:\n",
    "    - sentence: a sequence of ids for each word in the sentence\n",
    "- The function does the following:\n",
    "    - Obtains the embeddings for the input sequence\n",
    "    - passes them through an `LSTM` to get the respective hidden states; use the hidden state initialized in the function.\n",
    "    - projects them onto the tag-space using the FC layer\n",
    "- Make sure to reshape the embeddings of the words before sending them to the `BiLSTM`. The axes semantics are: `seq_len, mini_batch, embedding_dim`.\n",
    "- You can use the .view() method to reshape a tensor. You might need to use this, as the neural network components expect their inputs to have a certain shape. [check the pytorch doc on view](http://pytorch.org/docs/0.3.0/tensors.html?highlight=view#torch.Tensor.view)\n",
    "- **Tests**: ```test_bilstm.py: test_dlmodel_forward()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the random seed\n",
    "reload(bilstm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating your model\n",
    "torch.manual_seed(765);\n",
    "model = bilstm.BiLSTM(len(word_to_ix),tag_to_ix,embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'third', 'was', 'being', 'run', 'by', 'the', 'head', 'of', 'an', 'investment', 'firm', '.']\n"
     ]
    }
   ],
   "source": [
    "# preparing your first sentence to be an input\n",
    "words= X_tr[5]\n",
    "print(words)\n",
    "tags = Y_tr[5]\n",
    "sentence = bilstm.prepare_sequence(words, word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this calls the `forward()` function on the model, which returns the tag_scores for each tag for each particular token in the sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2635,  0.0640, -0.0517, -0.1657,  0.0004], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "lstm_feats = model(sentence)\n",
    "print (lstm_feats[0][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we provide the `predict()` function that returns the set of tags obtained for the specific input by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SYM', 'PART', 'PART', 'NOUN', 'NOUN', 'PART', 'PART', 'ADP', 'PART', 'ADJ', 'ADJ', 'PART', 'ADP']\n"
     ]
    }
   ],
   "source": [
    "tags = model.predict(sentence)\n",
    "print (tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the model for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Dev Accuracy: 0.7179324055666004\n",
      "Epoch 3: Dev Accuracy: 0.7860437375745527\n",
      "Epoch 5: Dev Accuracy: 0.8141153081510935\n",
      "Epoch 7: Dev Accuracy: 0.8294234592445328\n",
      "Epoch 9: Dev Accuracy: 0.8406361829025845\n"
     ]
    }
   ],
   "source": [
    "reload(bilstm);\n",
    "torch.manual_seed(765);\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "model, losses, accuracies = bilstm.train_model(loss, model, X_tr,Y_tr, word_to_ix, tag_to_ix, \n",
    "                                        X_dv, Y_dv, num_its=10, status_frequency=2, \n",
    "                                        optim_args = {'lr':0.1,'momentum':0}, param_file = 'best.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload(bilstm);\n",
    "bilstm.plot_results(losses, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 5.3**: (*5 points*)\n",
    "- As you can see from above, BiLSTM model performs worse than the Viterbi tagger.\n",
    "- In the next section we'll use pretrained embeddings like the following to improve your performance.\n",
    "- For now, tune the hyperparameters such that you obtain atleast `85.00%` accuracy on the dev set.\n",
    "- You can try changing the no. of iterations, optimizers, no. of LSTM layers, the hidden dimension units, the FC layer dimensions, etc...\n",
    "- **Tests**: ```test_bilstm.py: test_bilstm_dev_accuracy(), test_bilstm_test_accuracy()``` (you can't run this last one!)\n",
    "        \n",
    "**Remember: After training a model once, everytime when you rerun the train_model(...) function, the model is retrained on top of the previous parameters. If you want to start afresh, make sure the model is reinitialized and then trained.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using cuda\n",
    "If you want to run the code on a GPU: \n",
    "- make sure you convert the input and target Tensors to be cuda Variables like below:\n",
    "\n",
    "`inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())`\n",
    "- and convert your model to cuda:\n",
    "\n",
    " `model.cuda()`\n",
    "\n",
    "The following links would be useful: when you want to run your model on a GPU:  \n",
    " - [pytorch_doc](http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-on-gpu)\n",
    " - [mnist pytorch gpu example](https://github.com/pytorch/examples/blob/master/mnist/main.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Using Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Credit: Training Embeddings\n",
    "Which word embeddings we use can make a significant difference in performance of NN-based NLP tasks. You will implement Word2Vec using the Continuous Bag of Words (CBOW) architecture pre-train your own embeddings from a dataset and compare it to pre-trained embeddings available on the internet. Once you're done with this, you might investigate skip-gram models and other approaches to learning word embeddings to cement your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 6.1**: (*5 Points*)\n",
    "- The CBOW model learns to predict a word based on context of a pre-defined size. \n",
    "- Complete the ``cbow.build_context``function to build context vectors for a list documents.\n",
    "- **Input**\n",
    "    - List of tokenized documents.\n",
    "    - Context size.\n",
    "- **Output** \n",
    "    - List of (context, word) pairs, where context is a list containing the ``context_size`` words before and after ``word``. Note that no context is generated for words for which there is not ``context_size`` context available in the document. \n",
    "- **Tests**: ```test_cbow.py: test_build_context()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want our full vocabulary and word to index mapping.\n",
    "vocab, word_to_ix = most_common.get_word_to_ix(TRAIN_FILE)\n",
    "print ('words in the vocabulary: ',len(word_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(cbow)\n",
    "print(\"Original Sentence: \", X_tr[5])\n",
    "cbow.build_context([X_tr[5]], context_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build context for the entire training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_context = cbow.build_context(X_tr, context_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 6.2**: (*5 points*)\n",
    "- You will now implement the neural network model for training the CBOW, including the ``__init__`` method and the ``forward`` method.\n",
    "- For this deliverable you will implement the ``__init__`` method. \n",
    "- Your model will have 5 parts:\n",
    "    - ``self.embeddings`` - an ``nn.Embedding`` instance (see [nn.Embedding](https://pytorch.org/docs/stable/nn.html#embedding)) which will hold embeddings of size ``embedding_dim`` for each word in the vocabulary. \n",
    "    - ``self.linear1`` - A linear layer (see [nn.Linear](https://pytorch.org/docs/stable/nn.html#linear)) which each embedding will be fed in to, and whose output will be size 128. This is a hidden layer.\n",
    "    - This linear layer will use the ReLU activation function (see [nn.ReLU](https://pytorch.org/docs/stable/nn.html#relu))\n",
    "    - ``self.linear2`` - The output linear layer which will get input from the hidden layer and whose output will be the size of the vocabulary.\n",
    "    - The output layer will use the LogSoftmax activation function (see [nn.LogSoftmax](https://pytorch.org/docs/stable/nn.html#logsoftmax)). \n",
    "- There is no test for this deliverable - simply make sure your output in the next block matches mine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(cbow)\n",
    "cbow_embedding_dim = 64 # Output embedding size. \n",
    "cbow_model = cbow.CBOW(len(vocab), cbow_embedding_dim)\n",
    "print(cbow_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 6.3**: (*5 Points*)\n",
    "- For this deliverable you will implement the ``cbow.CBOW.forward`` method. \n",
    "- All this method has to do is:\n",
    "    - Sum the embeddings for the inputs, and turn the result into a 1-dimensional tensor (hint: use ``view``)\n",
    "    - Pass the result to ``linear1``, then its activation function.\n",
    "    - Pass the result of the above to ``linear2``, then the softmax function.\n",
    "    - Return the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train our CBOW model! This one is going to take quite a while if you aren't running it on a GPU with Cuda, so you might want to plan on running it overnight. You might try reducing the iterations to only a couple and adding a printout every thousand context vectors to make yourself a rough estimate for how long it'll take. I ran 25 iterations in about 3 hours in my run with CUDA on a GTX 1060.\n",
    "\n",
    "If it'll take too long to run, you can run it only a few iterations and see how you do, then run it using my model trained on 25 iterations (load it from ``data/cbow_model_state_64_25.torch``). Be sure your trained model saves using the below code so I can check it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" #change to cuda if you're using it!\n",
    "cbow_model.to(device)\n",
    "training_context_tensors = cbow.make_context_tensors(training_context, word_to_ix, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow.train_model(cbow_model, training_context_tensors, word_to_ix, iters=25, lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the filenames to better represent what you did - mine were 64 dimensional over 25 iterations.\n",
    "cbow_model.to(\"cpu\") # Move the model back to CPU if you've moded it for CUDA.\n",
    "torch.save(cbow_model, \"data/cbow_model_64_25_stu.torch\")\n",
    "torch.save(cbow_model.state_dict(), \"data/cbow_model_state_64_25_stu.torch\")\n",
    "cbow.write_polyglot_format(cbow_model, word_to_ix, \"data/cbow_model_polyglot_64_25_stu.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to, you can re-load a CBOW state dictionary later rather than re-training it with the following:\n",
    "cbow_model = cbow.CBOW(len(vocab), cbow_embedding_dim)\n",
    "cbow_model.load_state_dict(torch.load(\"data/cbow_model_state_64_25_stu.torch\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(emb1, emb2): #function to return the cosine similarity between the embeddings\n",
    "    return emb1.dot(emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_embeddings = cbow_model.embeddings.numpy()\n",
    "reload(cbow)\n",
    "\n",
    "terw2v = cbow_model.get_word_emdedding('terrible', word_to_ix)[0].detach().numpy()\n",
    "dirw2v = cbow_model.get_word_emdedding('dire', word_to_ix)[0].detach().numpy()\n",
    "sucw2v = cbow_model.get_word_emdedding('success', word_to_ix)[0].detach().numpy()\n",
    "\n",
    "print(sucw2v)\n",
    "print(cosine(dirw2v,terw2v)) # dire and terrible are similar\n",
    "print(cosine(dirw2v,sucw2v)) # dire and success aren't very similar\n",
    "print(cosine(terw2v,sucw2v)) # terrible and success seem similar, why do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test our embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bilstm)\n",
    "torch.manual_seed(765);\n",
    "embedding_dim=64\n",
    "hidden_dim=30\n",
    "model = bilstm.BiLSTM(len(word_to_ix),tag_to_ix,embedding_dim, hidden_dim, cbow_model.embeddings.to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "model, losses, accuracies = bilstm.train_model(loss, model, X_tr,Y_tr, word_to_ix, tag_to_ix, \n",
    "                                        X_dv, Y_dv, num_its=5, status_frequency=1, \n",
    "                                        optim_args = {'lr':0.01,'momentum':0}, param_file = 'best.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm.plot_results(losses, accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our trained model Word2Vec didn't seem to do as well as without pre-trained embeddings. Why do you think this is? Let's try some other pre-trained embeddings to see if perhaps our embeddings just aren't very good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained embeddings\n",
    "\n",
    "- *Polyglot* provides pre-trained word embeddings for many languages\n",
    "- You can download the english polyglot embeddings [here](https://sites.google.com/site/rmyeid/projects/polyglot).\n",
    "- Use the following helper function to load in the polyglot embeddings, which you can use to initialize your parameter embeddings in your model. You need to make sure your embedding_dim matches your embeddings size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bilstm)\n",
    "filename = 'data/polyglot-en.pkl'\n",
    "word_embeddings = bilstm.obtain_polyglot_embeddings(filename, word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let us have a look at these word embeddings. We can observe the cosine similarity between two word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(emb1, emb2): #function to return the cosine similarity between the embeddings\n",
    "    return emb1.dot(emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diremb = word_embeddings[word_to_ix['dire']]\n",
    "teremb = word_embeddings[word_to_ix['terrible']]\n",
    "succmb = word_embeddings[word_to_ix['success']]\n",
    "print(word_embeddings[word_to_ix['success']])\n",
    "print(cosine(diremb,teremb)) # dire and terrible are similar\n",
    "print(cosine(diremb,succmb))\n",
    "print(cosine(teremb,succmb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see these might be better than the ones we trained. Now, all we need to do is send in the word_embeddings when initializing your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(765);\n",
    "embedding_dim=64\n",
    "hidden_dim=30\n",
    "model = bilstm.BiLSTM(len(word_to_ix),tag_to_ix,embedding_dim, hidden_dim, word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "model, losses, accuracies = bilstm.train_model(loss, model, X_tr,Y_tr, word_to_ix, tag_to_ix, \n",
    "                                        X_dv, Y_dv, num_its=10, status_frequency=2, \n",
    "                                        optim_args = {'lr':0.01,'momentum':0}, param_file = 'best.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bilstm)\n",
    "bilstm.plot_results(losses, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(tagger_base);\n",
    "reload(bilstm);\n",
    "confusion = tagger_base.eval_model(model,'bilstm-dev-en.preds', word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (scorer.accuracy(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_base.apply_model(model,'bilstm-te-en.preds',word_to_ix, all_tags, testfile=TEST_FILE_HIDDEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you don't have en-ud-test.conllu, so you can't run this\n",
    "te_confusion = scorer.get_confusion(TEST_FILE,'bilstm-te-en.preds')\n",
    "print (scorer.accuracy(te_confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging for Norwegian\n",
    "Make sure your code runs on the Norwegian dataset as well. You may want to use the pretrained embeddings for norwegian to improve the performance of the model. However, there is no deliverable or test for this part.\n",
    "- Run the `BiLSTM` model on the norwegian dataset using the `BiLSTM` tagger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recalculating all_tags and tag_to_ix for the norwegian language\n",
    "if START_TAG in all_tags_nr:\n",
    "    all_tags_nr.remove(START_TAG)\n",
    "if END_TAG in all_tags_nr:\n",
    "    all_tags_nr.remove(END_TAG)\n",
    "\n",
    "tag_to_ix_nr={}\n",
    "for tag in all_tags_nr:\n",
    "    if tag not in tag_to_ix_nr:\n",
    "        tag_to_ix_nr[tag] = len(tag_to_ix_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recalculating the vocab for the norwegian language: obtains the most common 7600 words from the file\n",
    "vocab_nr, word_to_ix_nr = most_common.get_word_to_ix(NR_TRAIN_FILE, 7600)\n",
    "print ('words in the vocabulary: ',len(word_to_ix_nr))\n",
    "print (word_to_ix_nr[UNK])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading training data for norwegian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(preprocessing);\n",
    "X_tr_nr, Y_tr_nr = preprocessing.load_data(NR_TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading dev data for norwegian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dv_nr, Y_dv_nr = preprocessing.load_data(NR_DEV_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading test data for norwegian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te_nr, Y_te_nr = preprocessing.load_data(NR_TEST_FILE_HIDDEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the random seed\n",
    "torch.manual_seed(765);\n",
    "\n",
    "# initializing your model\n",
    "embedding_dim=30\n",
    "hidden_dim=30\n",
    "model = bilstm.BiLSTM(len(word_to_ix_nr),tag_to_ix_nr,embedding_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training your model for norwegian data\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "model, losses, accuracies = bilstm.train_model(loss, model, X_tr_nr,Y_tr_nr, word_to_ix_nr, tag_to_ix_nr, \n",
    "                                        X_dv_nr, Y_dv_nr, num_its=10, status_frequency=2, \n",
    "                                        optim_args = {'lr':0.2,'momentum':0}, param_file = 'best.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(bilstm);\n",
    "bilstm.plot_results(losses, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(tagger_base);\n",
    "reload(bilstm);\n",
    "confusion = tagger_base.eval_model(model,'bilstm-dev-nr.preds', word_to_ix_nr, \n",
    "                                   trainfile=NR_TRAIN_FILE, testfile=NR_DEV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (scorer.accuracy(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_base.apply_model(model,'bilstm-te-nr.preds',word_to_ix_nr, all_tags_nr, \n",
    "                        trainfile=NR_TRAIN_FILE, testfile=NR_TEST_FILE_HIDDEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you don't have no_bokmaal-ud-test.conllu, so you can't run this\n",
    "te_confusion = scorer.get_confusion(NR_TEST_FILE,'bilstm-te-nr.preds')\n",
    "print (scorer.accuracy(te_confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Improve Your Tagger!\n",
    "\n",
    "**Deliverable 7.1** (*10 points*)\n",
    "Improve your taggers performance to get atleast `91.00%` on the dev dataset for both English and Norwegian.\n",
    "You can use any model that you have implemented in this assignment: HMM, or BiLSTM.\n",
    "- HMM is unlikely to score high enough for full credit\n",
    "- So, iterating on BiLSTM might be better.    \n",
    "- Ideas: try varying the input (character-level features?), the optimizer, the model architecture, the pretrained embeddings, dropout, regularization, etc. Here are some more pretrained embeddings to try: [fastText](https://github.com/facebookresearch/fastText), [Polyglot](polyglot.readthedocs.io/en/latest/Embeddings.html), [word2vec](https://code.google.com/archive/p/word2vec/), [Glove](https://nlp.stanford.edu/projects/glove/).\n",
    "- For Norwegian, there are officially [two written forms of language](https://en.wikipedia.org/wiki/Norwegian_language): `Bokml` and `Nynorsk`. The dataset we are using is for the `Bokml` language. So, when using pretrained embeddings make sure to use it for that specific form.\n",
    "\n",
    "**Output files:** Make sure to name your files as follows: The unit tests will check for these files.\n",
    "- `model-dev-en.preds` ( Predictions for the dev dataset of english ) \n",
    "- `model-te-en.preds`  ( Predictions for the test dataset of english ) \n",
    "- `model-dev-nr.preds` ( Predictions for the dev dataset of norwegian ) \n",
    "- `model-te-nr.preds`  ( Predictions for the test dataset of norwegian ) \n",
    "\n",
    "**Tests**: `test_performance.py`: \n",
    "- `test_en_dev_accuracy()`\n",
    "- `test_en_test_accuracy()` #you cannot run this unit test.\n",
    "- `test_nr_dev_accuracy()`\n",
    "- `test_nr_test_accuracy()` #you cannot run this unit test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Dev Set (both languages)\n",
    "- $\\geq$ 90.5% (*0.15 points*)\n",
    "- $\\geq$ 91.0% (*0.25 points*)\n",
    "\n",
    "Test Set (both languages)\n",
    "- $\\geq$ 90.5% (*0.15 points*)\n",
    "- $\\geq$ 91.0% (*0.25 points*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 8. Compete! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Try to get the best accuracy possible on both `English` and `Norwegian` tagging.\n",
    "\n",
    "Some ideas:\n",
    "\n",
    "- Better features, such as characters\n",
    "- Better optimization\n",
    "- Number of Layers and Hidden units in the BiLSTM\n",
    "- More hidden layers in the Fully Connected Layer\n",
    "- Better loss function: like structured perceptron\n",
    "- Better preprocessing\n",
    "- Using Pretrained Embeddings like [fastText](https://github.com/facebookresearch/fastText), [Polyglot](polyglot.readthedocs.io/en/latest/Embeddings.html), [word2vec](https://code.google.com/archive/p/word2vec/), [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "- Dropout or other regularization scheme\n",
    "- Go crazy and implement a Conditional Random Field!\n",
    "\n",
    "**Output files:** Make sure to name your files as follows:\n",
    "\n",
    "- `compete-dev-en.preds` ( Predictions for the dev dataset of english ) \n",
    "- `compete-te-en.preds`  ( Predictions for the test dataset of english ) \n",
    "- `compete-dev-nr.preds` ( Predictions for the dev dataset of norwegian ) \n",
    "- `compete-te-nr.preds`  ( Predictions for the test dataset of norwegian ) \n",
    "\n",
    "The unit tests will check for these files. To get the extra credit, you must submit these files.\n",
    "\n",
    "**Rubric**\n",
    "\n",
    "- Top 3 final score in English: +5%\n",
    "- Top 3 final score in Norwegian: +5%\n",
    "- Beat my best final score in English: +5%\n",
    "- Best my best final score in Norwegian: +5%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
